ご依頼のページの内容を取得して、正確に和訳いたします。以下、ご依頼のページを日本語に翻訳いたします。

---

# エージェントを評価する理由

従来のソフトウェア開発では、単体テストと統合テストにより、コードが期待通りに機能し、変更を加えても安定性が保たれるという確信が得られます。これらのテストは明確な「合格/不合格」のシグナルを提供し、さらなる開発の指針となります。しかし、LLMエージェントは変動性をもたらすため、従来のテスト手法では不十分です。

モデルの確率的な性質により、決定論的な「合格/不合格」のアサーションは、エージェントのパフォーマンスを評価するのに適さないことがよくあります。代わりに、最終出力とエージェントの軌跡(解決策に到達するまでに取った一連のステップ)の両方について、質的な評価が必要です。これには、エージェントの意思決定の質、その推論プロセス、そして最終結果の評価が含まれます。

これは設定に多くの追加作業が必要に思えるかもしれませんが、評価を自動化する投資は急速に報われます。プロトタイプを超えて進むつもりであれば、これは非常に推奨されるベストプラクティスです。

# エージェント評価の準備

エージェント評価を自動化する前に、明確な目標と成功基準を定義します:

- **成功の定義**: エージェントにとって成功とみなされる結果とは何か?
- **重要なタスクの特定**: エージェントが達成しなければならない必須のタスクは何か?
- **関連するメトリクスの選択**: パフォーマンスを測定するためにどのようなメトリクスを追跡するか?

これらの検討事項は、評価シナリオの作成を導き、実際の展開におけるエージェントの行動の効果的な監視を可能にします。

# 何を評価するか?

概念実証から本番環境に対応したAIエージェントへの橋渡しをするには、堅牢で自動化された評価フレームワークが不可欠です。生成モデルの評価では主に最終出力に焦点が当てられますが、エージェントの評価では意思決定プロセスのより深い理解が必要です。エージェントの評価は2つのコンポーネントに分解できます:

- **軌跡とツールの使用の評価**: エージェントが解決策に到達するまでに取るステップを分析します。これには、ツールの選択、戦略、およびアプローチの効率性が含まれます。
- **最終応答の評価**: エージェントの最終出力の質、関連性、正確性を評価します。

軌跡とは、エージェントがユーザーに戻る前に取ったステップのリストに過ぎません。これを、エージェントが取ると期待されるステップのリストと比較することができます。

## 軌跡とツール使用の評価

ユーザーに応答する前に、エージェントは通常、一連のアクションを実行します。これを「軌跡」と呼びます。例えば、ユーザー入力をセッション履歴と比較して用語を明確化したり、ポリシー文書を検索したり、ナレッジベースを検索したり、チケットを保存するためにAPIを呼び出したりすることがあります。これを「軌跡」と呼びます。エージェントのパフォーマンスを評価するには、その実際の軌跡を期待される、または理想的な軌跡と比較する必要があります。この比較により、エージェントのプロセスにおけるエラーや非効率性を明らかにすることができます。期待される軌跡はグラウンドトゥルース(正解)を表しており、エージェントが取るべきと予想されるステップのリストです。

例:

```python
# 軌跡評価は以下を比較します
expected_steps = ["determine_intent", "use_tool", "review_results", "report_generation"]
actual_steps = ["determine_intent", "use_tool", "review_results", "report_generation"]
```

いくつかのグラウンドトゥルースベースの軌跡評価が存在します:

- **完全一致**: 理想的な軌跡との完全な一致を要求します。
- **順序一致**: 正しい順序で正しいアクションを要求し、余分なアクションを許可します。
- **任意順序一致**: 任意の順序で正しいアクションを要求し、余分なアクションを許可します。
- **精度**: 予測されたアクションの関連性/正確性を測定します。
- **再現率**: 予測において捕捉された必須アクションの数を測定します。
- **単一ツール使用**: 特定のアクションの包含をチェックします。

適切な評価メトリクスの選択は、エージェントの特定の要件と目標に依存します。例えば、高リスクのシナリオでは完全一致が重要かもしれませんが、より柔軟な状況では順序一致や任意順序一致で十分かもしれません。

# ADKでの評価の仕組み

ADKは、事前定義されたデータセットと評価基準に対してエージェントのパフォーマンスを評価する2つの方法を提供します。概念的には似ていますが、処理できるデータの量が異なり、これが通常、各々の適切なユースケースを決定します。

## 第一のアプローチ: テストファイルの使用

このアプローチでは、個々のテストファイルを作成します。各ファイルは単一のシンプルなエージェントモデルインタラクション(セッション)を表します。これは、アクティブなエージェント開発中に最も効果的であり、単体テストの一形態として機能します。これらのテストは迅速な実行のために設計されており、シンプルなセッションの複雑さに焦点を当てるべきです。各テストファイルには1つのセッションが含まれ、複数のターンで構成されることがあります。ターンは、ユーザーとエージェント間の単一のインタラクションを表します。各ターンには以下が含まれます:

- **ユーザーコンテンツ**: ユーザーが発行したクエリ。
- **期待される中間ツール使用軌跡**: ユーザークエリに正しく応答するためにエージェントが行うと期待されるツール呼び出し。
- **期待される中間エージェント応答**: これらは、エージェント(またはサブエージェント)が最終回答の生成に向かう際に生成する自然言語の応答です。これらの自然言語応答は通常、ルートエージェントがサブエージェントに依存して目標を達成するマルチエージェントシステムの副産物です。これらの中間応答は、エンドユーザーにとって興味の対象である場合もそうでない場合もありますが、システムの開発者/所有者にとっては非常に重要です。なぜなら、エージェントが最終応答を生成するために正しい経路を通ったという確信を与えるからです。
- **最終応答**: エージェントからの期待される最終応答。

ファイルには任意の名前を付けることができます。例えば `evaluation.test.json` です。フレームワークは `.test.json` サフィックスのみをチェックし、ファイル名の前の部分は制約されません。以下は、いくつかの例を含むテストファイルです:

**注意**: テストファイルは現在、正式なPydanticデータモデルによってサポートされています。2つの主要なスキーマファイルは [Eval Set](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_set.py) と [Eval Case](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_case.py) です。

(注: コメントは説明目的で含まれており、JSONを有効にするには削除する必要があります。)

```json
# このドキュメントを読みやすくするために、いくつかのフィールドは削除されています。
{
  "eval_set_id": "home_automation_agent_light_on_off_set",
  "name": "",
  "description": "これは、エージェントの `x` 動作の単体テストに使用される評価セットです",
  "eval_cases": [
    {
      "eval_id": "eval_case_id",
      "conversation": [
        {
          "invocation_id": "b7982664-0ab6-47cc-ab13-326656afdf75", # 呼び出しの一意の識別子。
          "user_content": { # この呼び出しでユーザーが提供したコンテンツ。これがクエリです。
            "parts": [
              {
                "text": "寝室のdevice_2をオフにして。"
              }
            ],
            "role": "user"
          },
          "final_response": { # ベンチマークの参照として機能するエージェントからの最終応答。
            "parts": [
              {
                "text": "device_2のステータスをオフに設定しました。"
              }
            ],
            "role": "model"
          },
          "intermediate_data": {
            "tool_uses": [ # 時系列順のツール使用軌跡。
              {
                "args": {
                  "location": "Bedroom",
                  "device_id": "device_2",
                  "status": "OFF"
                },
                "name": "set_device_info"
              }
            ],
            "intermediate_responses": [] # サブエージェントの中間応答。
          },
        }
      ],
      "session_input": { # 初期セッション入力。
        "app_name": "home_automation_agent",
        "user_id": "test_user",
        "state": {}
      },
    }
  ],
}
```

テストファイルはフォルダに整理できます。オプションで、フォルダには評価基準を指定する `test_config.json` ファイルを含めることもできます。

### Pydanticスキーマによってサポートされていないテストファイルを移行する方法

**注意**: テストファイルが [EvalSet](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_set.py) スキーマファイルに準拠していない場合、このセクションが関連します。

既存の `*.test.json` ファイルをPydanticベースのスキーマに移行するには、`AgentEvaluator.migrate_eval_data_to_new_schema` を使用してください。

このユーティリティは、現在のテストデータファイルとオプションの初期セッションファイルを取り、新しい形式でシリアライズされたデータを含む単一の出力JSONファイルを生成します。新しいスキーマがより統合的であることを考えると、古いテストデータファイルと初期セッションファイルの両方は無視(または削除)できます。

## 第二のアプローチ: 評価セットファイルの使用

評価セットアプローチは、エージェントモデルのインタラクションを評価するために「評価セット」と呼ばれる専用のデータセットを利用します。テストファイルと同様に、評価セットには例のインタラクションが含まれます。ただし、評価セットには複数の、潜在的に長いセッションを含めることができ、複雑なマルチターンの会話をシミュレートするのに理想的です。複雑なセッションを表現する能力により、評価セットは統合テストに適しています。これらのテストは、より広範な性質のため、単体テストよりも実行頻度が低くなります。

評価セットファイルには、複数の「評価」が含まれ、各々が異なるセッションを表します。各評価は1つ以上の「ターン」で構成され、ユーザークエリ、期待されるツール使用、期待される中間エージェント応答、および参照応答が含まれます。これらのフィールドは、テストファイルアプローチと同じ意味を持ちます。各評価は一意の名前で識別されます。さらに、各評価には関連する初期セッション状態が含まれます。

評価セットを手動で作成することは複雑になる可能性があるため、関連するセッションをキャプチャし、評価セット内の評価に簡単に変換するのに役立つUIツールが提供されています。以下で、評価用のWeb UIの使用について詳しく学びます。以下は、2つのセッションを含む評価セットの例です。

**注意**: 評価セットファイルは現在、正式なPydanticデータモデルによってサポートされています。2つの主要なスキーマファイルは [Eval Set](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_set.py) と [Eval Case](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_case.py) です。

(注: コメントは説明目的で含まれており、JSONを有効にするには削除する必要があります。)

```json
# このドキュメントを読みやすくするために、いくつかのフィールドは削除されています。
{
  "eval_set_id": "eval_set_example_with_multiple_sessions",
  "name": "複数のセッションを含む評価セット",
  "description": "この評価セットは、評価セットが複数のセッションを持つことができることを示す例です。",
  "eval_cases": [
    {
      "eval_id": "session_01",
      "conversation": [
        {
          "invocation_id": "e-0067f6c4-ac27-4f24-81d7-3ab994c28768",
          "user_content": {
            "parts": [
              {
                "text": "何ができますか?"
              }
            ],
            "role": "user"
          },
          "final_response": {
            "parts": [
              {
                "text": "さまざまなサイズのサイコロを振ったり、数字が素数かどうかを確認したりできます。"
              }
            ],
            "role": null
          },
          "intermediate_data": {
            "tool_uses": [],
            "intermediate_responses": []
          },
        }
      ],
      "session_input": {
        "app_name": "hello_world",
        "user_id": "user",
        "state": {}
      },
    },
    {
      "eval_id": "session_02",
      "conversation": [
        {
          "invocation_id": "e-92d34c6d-0a1b-452a-ba90-33af2838647a",
          "user_content": {
            "parts": [
              {
                "text": "19面のサイコロを振って"
              }
            ],
            "role": "user"
          },
          "final_response": {
            "parts": [
              {
                "text": "17が出ました。"
              }
            ],
            "role": null
          },
          "intermediate_data": {
            "tool_uses": [],
            "intermediate_responses": []
          },
        },
        {
          "invocation_id": "e-bf8549a1-2a61-4ecc-a4ee-4efbbf25a8ea",
          "user_content": {
            "parts": [
              {
                "text": "10面のサイコロを2回振って、9が素数かどうか確認して"
              }
            ],
            "role": "user"
          },
          "final_response": {
            "parts": [
              {
                "text": "サイコロで4と7が出て、9は素数ではありません。\n"
              }
            ],
            "role": null
          },
          "intermediate_data": {
            "tool_uses": [
              {
                "id": "adk-1a3f5a01-1782-4530-949f-07cf53fc6f05",
                "args": {
                  "sides": 10
                },
                "name": "roll_die"
              },
              {
                "id": "adk-52fc3269-caaf-41c3-833d-511e454c7058",
                "args": {
                  "sides": 10
                },
                "name": "roll_die"
              },
              {
                "id": "adk-5274768e-9ec5-4915-b6cf-f5d7f0387056",
                "args": {
                  "nums": [
                    9
                  ]
                },
                "name": "check_prime"
              }
            ],
            "intermediate_responses": [
              [
                "data_processing_agent",
                [
                  {
                    "text": "10面のサイコロを2回振りました。最初の目は5で、2回目の目は3です。\n"
                  }
                ]
              ]
            ]
          },
        }
      ],
      "session_input": {
        "app_name": "hello_world",
        "user_id": "user",
        "state": {}
      },
    }
  ],
}
```

### Pydanticスキーマによってサポートされていない評価セットファイルを移行する方法

**注意**: 評価セットファイルが [EvalSet](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_set.py) スキーマファイルに準拠していない場合、このセクションが関連します。

評価セットデータを誰が管理しているかに基づいて、2つの経路があります:

1. **ADK UIによって管理される評価セットデータ** ADK UIを使用して評価セットデータを管理している場合、アクションは必要ありません。

2. **手動で開発・管理され、ADK評価CLIで使用される評価セットデータ** 移行ツールが開発中です。それまでの間、ADK評価CLIコマンドは引き続き古い形式のデータをサポートします。

## 評価基準

評価基準は、評価セットに対してエージェントのパフォーマンスがどのように測定されるかを定義します。以下のメトリクスがサポートされています:

- **tool_trajectory_avg_score**: このメトリクスは、評価中のエージェントの実際のツール使用を、`expected_tool_use` フィールドで定義された期待されるツール使用と比較します。一致する各ツール使用ステップは1のスコアを受け取り、不一致は0のスコアを受け取ります。最終スコアはこれらの一致の平均であり、ツール使用軌跡の精度を表します。

- **response_match_score**: このメトリクスは、エージェントの最終的な自然言語応答を、`reference` フィールドに保存された期待される最終応答と比較します。2つの応答間の類似性を計算するために、[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric))メトリクスを使用します。

評価基準が提供されない場合、以下のデフォルト設定が使用されます:

- **tool_trajectory_avg_score**: デフォルトは1.0で、ツール使用軌跡の100%一致を要求します。
- **response_match_score**: デフォルトは0.8で、エージェントの自然言語応答に小さな誤差の余地を許容します。

以下は、カスタム評価基準を指定する `test_config.json` ファイルの例です:

# ADKで評価を実行する方法

開発者として、以下の方法でADKを使用してエージェントを評価できます:

- **Web ベースのUI (`adk web`)**: Webベースのインターフェースを介してインタラクティブにエージェントを評価します。
- **プログラム的に (`pytest`)**: `pytest` とテストファイルを使用して、テストパイプラインに評価を統合します。
- **コマンドラインインターフェース (`adk eval`)**: コマンドラインから直接、既存の評価セットファイルで評価を実行します。

## 1. `adk web` - Web UIで評価を実行

Web UIは、エージェントを評価し、評価データセットを生成し、エージェントの動作を詳細に検査するインタラクティブな方法を提供します。

### ステップ1: テストケースを作成して保存

- 次のコマンドを実行してWebサーバーを起動します:
  ```
  adk web <エージェントフォルダへのパス>
  ```
- Webインターフェースで、エージェントを選択してインタラクトし、セッションを作成します。
- インターフェースの右側にある「Eval」タブに移動します。
- 新しい評価セットを作成するか、既存のものを選択します。
- 「現在のセッションを追加」をクリックして、会話を新しい評価ケースとして保存します。

### ステップ2: テストケースを表示および編集

ケースが保存されたら、リストでそのIDをクリックして検査できます。変更を加えるには、「現在の評価ケースを編集」アイコン(鉛筆)をクリックします。このインタラクティブビューでは以下が可能です:

- エージェントのテキスト応答を修正してテストシナリオを改善する。
- 会話から個々のエージェントメッセージを削除する。
- 必要なくなった場合は、評価ケース全体を削除する。

### ステップ3: カスタムメトリクスで評価を実行

- 評価セットから1つ以上のテストケースを選択します。
- 「評価を実行」をクリックします。評価メトリクスダイアログが表示されます。
- ダイアログで、スライダーを使用して以下のしきい値を設定します:
  - ツール軌跡平均スコア
  - 応答一致スコア
- 「開始」をクリックして、カスタム基準を使用して評価を実行します。評価履歴には、各実行で使用されたメトリクスが記録されます。

### ステップ4: 結果を分析

実行が完了したら、結果を分析できます:

- **実行失敗を分析**: 合格または不合格の結果をクリックします。失敗の場合、`Fail` ラベルにカーソルを合わせると、実際の出力と期待される出力の並列比較、および失敗の原因となったスコアが表示されます。

### トレースビューでのデバッグ

ADK Web UIには、エージェントの動作をデバッグするための強力な「Trace」タブが含まれています。この機能は、評価中だけでなく、あらゆるエージェントセッションで利用できます。

Traceタブは、エージェントの実行フローを検査するための詳細でインタラクティブな方法を提供します。トレースはユーザーメッセージごとに自動的にグループ化され、イベントのチェーンを簡単に追跡できます。

各トレース行はインタラクティブです:

- トレース行にカーソルを合わせると、チャットウィンドウの対応するメッセージがハイライトされます。
- トレース行をクリックすると、4つのタブを持つ詳細な検査パネルが開きます:
  - **Event**: 生のイベントデータ。
  - **Request**: モデルに送信されたリクエスト。
  - **Response**: モデルから受信した応答。
  - **Graph**: ツール呼び出しとエージェントロジックフローの視覚的表現。

トレースビューの青い行は、そのインタラクションからイベントが生成されたことを示します。これらの青い行をクリックすると、下部のイベント詳細パネルが開き、エージェントの実行フローに関するより深い洞察が得られます。

## 2. `pytest` - プログラム的にテストを実行

`pytest` を使用して、統合テストの一部としてテストファイルを実行することもできます。

### コマンド例

### テストコード例

以下は、単一のテストファイルを実行する `pytest` テストケースの例です:

```python
from google.adk.evaluation.agent_evaluator import AgentEvaluator
import pytest

@pytest.mark.asyncio
async def test_with_single_test_file():
    """セッションファイルを介してエージェントの基本的な能力をテストします。"""
    await AgentEvaluator.evaluate(
        agent_module="home_automation_agent",
        eval_dataset_file_path_or_dir="tests/integration/fixture/home_automation_agent/simple_test.test.json",
    )
```

このアプローチにより、エージェント評価をCI/CDパイプラインや大規模なテストスイートに統合できます。テストの初期セッション状態を指定したい場合は、セッションの詳細をファイルに保存し、それを `AgentEvaluator.evaluate` メソッドに渡すことで実現できます。

## 3. `adk eval` - CLIで評価を実行

コマンドラインインターフェース(CLI)を介して評価セットファイルの評価を実行することもできます。これはUIで実行されるのと同じ評価を実行しますが、自動化に役立ちます。つまり、このコマンドを通常のビルド生成および検証プロセスの一部として追加できます。

以下がコマンドです:

```bash
adk eval \
  <AGENT_MODULE_FILE_PATH> \
  <EVAL_SET_FILE_PATH> \
  [--config_file_path=<PATH_TO_TEST_JSON_CONFIG_FILE>] \
  [--print_detailed_results]
```

例:

```bash
adk eval \
  samples_for_testing/hello_world \
  samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json
```

各コマンドライン引数の詳細:

- **AGENT_MODULE_FILE_PATH**: 「agent」という名前のモジュールを含む `__init__.py` ファイルへのパス。「agent」モジュールには `root_agent` が含まれています。
- **EVAL_SET_FILE_PATH**: 評価ファイルへのパス。1つ以上の評価セットファイルパスを指定できます。各ファイルについて、すべての評価がデフォルトで実行されます。評価セットから特定の評価のみを実行したい場合は、まず評価名のカンマ区切りリストを作成し、それをコロンで区切って評価セットファイル名のサフィックスとして追加します:
  - 例: `sample_eval_set_file.json:eval_1,eval_2,eval_3`
  - これにより、sample_eval_set_file.jsonからeval_1、eval_2、eval_3のみが実行されます。
- **CONFIG_FILE_PATH**: 設定ファイルへのパス。
- **PRINT_DETAILED_RESULTS**: 詳細な結果をコンソールに出力します。

---

以上が翻訳となります。